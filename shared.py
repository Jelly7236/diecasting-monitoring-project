# shared.py
from pathlib import Path
import matplotlib.pyplot as plt
import pandas as pd
import joblib
import shap
from matplotlib import font_manager as fm
import os
import json
import sys
from shiny import reactive


from models.FinalModel.smote_sampler import MajorityVoteSMOTENC

# app.pyê°€ ìˆëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ê´€ë¦¬
app_dir = Path(__file__).parent
# ë°ì´í„° ê²½ë¡œ
data_dir = app_dir / "data"
models_dir = app_dir / "models"
fonts_dir = app_dir / "www" / "fonts"

def setup_korean_font():
    """ë¡œì»¬ + ë¦¬ëˆ…ìŠ¤ ì„œë²„ì—ì„œ ëª¨ë‘ í•œê¸€ ê¹¨ì§ ë°©ì§€"""
    font_candidates = []

    # 1. í”„ë¡œì íŠ¸ í°íŠ¸ (ê¶Œì¥)
    font_path = Path(__file__).parent / "www" / "fonts" / "NotoSansKR-Regular.ttf"
    if font_path.exists():
        font_prop = fm.FontProperties(fname=str(font_path))
        plt.rcParams['font.family'] = font_prop.get_name()
        fm.fontManager.addfont(str(font_path))
        print(f"âœ… ë‚´ë¶€ í°íŠ¸ ì ìš©: {font_prop.get_name()}")
        return

    # 2. ë¡œì»¬ OSë³„ ê¸°ë³¸ í°íŠ¸
    font_candidates = ["Malgun Gothic", "AppleGothic", "NanumGothic", "Noto Sans KR"]

    for font in font_candidates:
        if font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):
            plt.rcParams['font.family'] = font
            print(f"âœ… ì‹œìŠ¤í…œ í°íŠ¸ ì ìš©: {font}")
            return

    # 3. fallback
    plt.rcParams['font.family'] = "DejaVu Sans"
    print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•´ DejaVu Sansë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")

    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False
setup_korean_font()


# plt.rcParams['axes.unicode_minus'] = False

# Data Load
streaming_df = pd.read_csv(data_dir / "train2.csv")
prediction_state = reactive.Value(pd.DataFrame()) ### ì—¬ê¸°ì¶”ê°€
current_state = reactive.Value(pd.DataFrame())  # âœ… ì‹¤ì‹œê°„ ê³µì • ë°ì´í„° (ì„¼ì„œ ê¸°ë°˜)

# ì‹œê°„ ì¹¼ëŸ¼ í†µí•© (ì˜µì…˜)
streaming_df["datetime"] = pd.to_datetime(streaming_df["date"] + " " + streaming_df["time"])
streaming_df = streaming_df.sort_values("datetime").reset_index(drop=True)


# ì´ìƒì¹˜ ì œê±° ë°ì´í„°
df2 = pd.read_csv(data_dir / "outlier_remove_data2.csv")


rf_models = {
    "8412": joblib.load(models_dir / "RandomForest" /"rf_mold_8412.pkl"),
    "8573": joblib.load(models_dir / "RandomForest" /"rf_mold_8573.pkl"),
    "8600": joblib.load(models_dir / "RandomForest" /"rf_mold_8600.pkl"),
    "8722": joblib.load(models_dir / "RandomForest" /"rf_mold_8722.pkl"),
    "8917": joblib.load(models_dir / "RandomForest" /"rf_mold_8917.pkl"),
    
}

rf_explainers = {
    "8412": shap.TreeExplainer(rf_models["8412"].named_steps["model"]),
    "8573": shap.TreeExplainer(rf_models["8573"].named_steps["model"]),
    "8600": shap.TreeExplainer(rf_models["8600"].named_steps["model"]),
    "8722": shap.TreeExplainer(rf_models["8722"].named_steps["model"]),
    "8917": shap.TreeExplainer(rf_models["8917"].named_steps["model"]),
    
}

# shared.pyì— ì¶”ê°€
rf_preprocessors = {
    "8412": rf_models["8412"].named_steps["preprocess"],
    "8573": rf_models["8573"].named_steps["preprocess"],
    "8600": rf_models["8600"].named_steps["preprocess"],
    "8722": rf_models["8722"].named_steps["preprocess"],
    "8917": rf_models["8917"].named_steps["preprocess"],
}

# ì „ì²˜ë¦¬ëœ ì»¬ëŸ¼ëª… â†’ ì›ë˜ ë³€ìˆ˜ëª…
feature_name_map = {
    "num__molten_temp": "molten_temp",
    "num__molten_volume": "molten_volume",
    "num__sleeve_temperature": "sleeve_temperature",
    "num__EMS_operation_time": "EMS_operation_time",
    "num__cast_pressure": "cast_pressure",
    "num__biscuit_thickness": "biscuit_thickness",
    "num__low_section_speed": "low_section_speed",
    "num__high_section_speed": "high_section_speed",
    "num__physical_strength": "physical_strength",
    "num__upper_mold_temp1": "upper_mold_temp1",
    "num__upper_mold_temp2": "upper_mold_temp2",
    "num__lower_mold_temp1": "lower_mold_temp1",
    "num__lower_mold_temp2": "lower_mold_temp2",
    "num__Coolant_temperature": "Coolant_temperature",
    "num__facility_operation_cycleTime": "facility_operation_cycleTime",
    "num__production_cycletime": "production_cycletime",
    "num__count": "count",
    "cat__working_ê°€ë™": "working=ê°€ë™",
    "cat__working_ì •ì§€": "working=ì •ì§€",
    "cat__tryshot_signal_A": "tryshot_signal=A",
    "cat__tryshot_signal_D": "tryshot_signal=D",
}

feature_name_map_kor = {
    "num__molten_temp": "ìš©íƒ• ì˜¨ë„(â„ƒ)",
    "num__molten_volume": "ìš©íƒ• ë¶€í”¼",
    "num__sleeve_temperature": "ìŠ¬ë¦¬ë¸Œ ì˜¨ë„(â„ƒ)",
    "num__EMS_operation_time": "EMS ì‘ë™ì‹œê°„(s)",
    "num__cast_pressure": "ì£¼ì¡° ì••ë ¥(bar)",
    "num__biscuit_thickness": "ë¹„ìŠ¤í‚· ë‘ê»˜(mm)",
    "num__low_section_speed": "ì €ì† êµ¬ê°„ ì†ë„",
    "num__high_section_speed": "ê³ ì† êµ¬ê°„ ì†ë„",
    "num__physical_strength": "í˜•ì²´ë ¥",
    "num__upper_mold_temp1": "ìƒí˜• ì˜¨ë„1(â„ƒ)",
    "num__upper_mold_temp2": "ìƒí˜• ì˜¨ë„2(â„ƒ)",
    "num__lower_mold_temp1": "í•˜í˜• ì˜¨ë„1(â„ƒ)",
    "num__lower_mold_temp2": "í•˜í˜• ì˜¨ë„2(â„ƒ)",
    "num__coolant_temp": "ëƒ‰ê°ìˆ˜ ì˜¨ë„(â„ƒ)",
    "num__facility_operation_cycleTime": "ì„¤ë¹„ ê°€ë™ ì‚¬ì´í´íƒ€ì„",
    "num__production_cycletime": "ìƒì‚° ì‚¬ì´í´íƒ€ì„",
    "num__count": "ìƒì‚° íšŸìˆ˜",
    "cat__working_ê°€ë™": "ì‘ì—… ì—¬ë¶€=ê°€ë™",
    "cat__working_ì •ì§€": "ì‘ì—… ì—¬ë¶€=ì •ì§€",
    "cat__tryshot_signal_A": "íŠ¸ë¼ì´ìƒ· ì‹ í˜¸=A",
    "cat__tryshot_signal_D": "íŠ¸ë¼ì´ìƒ· ì‹ í˜¸=D",
}

name_map_kor = {
    # ë©”íƒ€/ì‹ë³„ì
    "id": "í–‰ ID",
    "line": "ì‘ì—… ë¼ì¸",
    "name": "ì œí’ˆëª…",
    "mold_name": "ê¸ˆí˜•ëª…",
    "time": "ìˆ˜ì§‘ ì‹œê°„",
    "date": "ìˆ˜ì§‘ ì¼ì",
    "registration_time": "ë“±ë¡ ì¼ì‹œ",

    # ìƒì‚° ê´€ë ¨
    "count": "ìƒì‚° íšŸìˆ˜",
    "working": "ì‘ì—… ì—¬ë¶€",
    "emergency_stop": "ë¹„ìƒì •ì§€ ì—¬ë¶€",
    "passorfail": "ì–‘/ë¶ˆ íŒì • ê²°ê³¼",
    "tryshot_signal": "íŠ¸ë¼ì´ìƒ· ì—¬ë¶€",
    "mold_code": "ê¸ˆí˜• ì½”ë“œ",
    "heating_furnace": "ê°€ì—´ë¡œ êµ¬ë¶„",

    # ê³µì • ë³€ìˆ˜
    "molten_temp": "ìš©íƒ• ì˜¨ë„",
    "molten_volume": "ìš©íƒ• ë¶€í”¼",
    "sleeve_temperature": "ìŠ¬ë¦¬ë¸Œ ì˜¨ë„",
    "EMS_operation_time": "EMS ì‘ë™ì‹œê°„",
    "cast_pressure": "ì£¼ì¡° ì••ë ¥(bar)",
    "biscuit_thickness": "ë¹„ìŠ¤í‚· ë‘ê»˜(mm)",
    "low_section_speed": "ì €ì† êµ¬ê°„ ì†ë„",
    "high_section_speed": "ê³ ì† êµ¬ê°„ ì†ë„",
    "physical_strength": "í˜•ì²´ë ¥",

    # ê¸ˆí˜• ì˜¨ë„
    "upper_mold_temp1": "ìƒí˜• ì˜¨ë„1",
    "upper_mold_temp2": "ìƒí˜• ì˜¨ë„2",
    "upper_mold_temp3": "ìƒí˜• ì˜¨ë„3",
    "lower_mold_temp1": "í•˜í˜• ì˜¨ë„1",
    "lower_mold_temp2": "í•˜í˜• ì˜¨ë„2",
    "lower_mold_temp3": "í•˜í˜• ì˜¨ë„3",

    # ëƒ‰ê° ê´€ë ¨
    "Coolant_temperature": "ëƒ‰ê°ìˆ˜ ì˜¨ë„",

    # ì‚¬ì´í´ ê´€ë ¨
    "facility_operation_cycleTime": "ì„¤ë¹„ ê°€ë™ ì‚¬ì´í´íƒ€ì„",
    "production_cycletime": "ìƒì‚° ì‚¬ì´í´íƒ€ì„",

    # íŒŒìƒ ë³€ìˆ˜
    "day": "ì¼",
    "month": "ì›”",
    "weekday": "ìš”ì¼"
}

mold_list = ["8412", "8573", "8600", "8722", "8917"]

# Isolation Forest ëª¨ë¸ ë¡œë“œ
iso_dir = models_dir / "isolation forest" / "saved_models(IF)"
iso_meta_path = models_dir / "isolation forest" /"saved_models(IF)" / "isoforest_20251015_150622_meta.json"

iso_models = {}

for mold in mold_list:
    model_path = iso_dir / f"isolation_forest_{mold}.pkl"
    try:
        iso_models[mold] = joblib.load(model_path)
        print(f"âœ… IsolationForest ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path.name}")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({mold}): {e}")

try:
    with open(iso_meta_path, "r", encoding="utf-8") as f:
        iso_meta = json.load(f)
    iso_features = iso_meta.get("features", [])
    print(f"âœ… IsolationForest ë©”íƒ€ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {iso_meta_path.name}")
except Exception as e:
    iso_meta, iso_features = None, []
    print(f"âš ï¸ IsolationForest ë©”íƒ€ ë¡œë“œ ì‹¤íŒ¨: {e}")

print(f"\nì´ ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(iso_models)}ê°œ")
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ ìˆ˜: {len(iso_features)}ê°œ â†’ {iso_features if iso_features else 'ë©”íƒ€ ì—†ìŒ'}")

# RandomForestTimeSeries ëª¨ë¸ ë¡œë“œ
rft_dir = models_dir / "RandomForestTimeSeries"

rft_models = {}

# âœ… scikit-learn ë‚´ë¶€ í´ë˜ìŠ¤ í˜¸í™˜ íŒ¨ì¹˜
import sklearn.compose._column_transformer as ct
sys.modules["__main__"].MajorityVoteSMOTENC = MajorityVoteSMOTENC

if not hasattr(ct, "_RemainderColsList"):
    class _RemainderColsList(list):
        """Dummy placeholder for sklearn 1.6.x compatibility"""
        pass
    ct._RemainderColsList = _RemainderColsList
    print("âœ… sklearn.compose._column_transformer._RemainderColsList ë”ë¯¸ í´ë˜ìŠ¤ ë“±ë¡ ì™„ë£Œ")
    
for mold in mold_list:
    model_path = rft_dir / f"model_{mold}.pkl"
    try:
        rft_models[mold] = joblib.load(model_path)
        print(f"âœ… RandomForestTimeSeries ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path.name}")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({mold}): {e}")

print(f"\nì´ ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(rft_models)}ê°œ")

### ë‹¨ë³€ëŸ‰ ê´€ë¦¬ë„ ARIMA ëª¨ë¸ ë¡œë“œ
BASE_DIR = Path(__file__).parent
ARIMA_INFO_PATH = BASE_DIR / "data" / "arima_model_info_updated.csv"

arima_models = {}

try:
    arima_info = pd.read_csv(ARIMA_INFO_PATH)
    print(f"ğŸ“„ ARIMA ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(arima_info)}í–‰")
except FileNotFoundError:
    print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ â†’ {ARIMA_INFO_PATH}")
    arima_info = pd.DataFrame()

for _, row in arima_info.iterrows():
    mold = str(row.get("mold_code", "")).strip()
    var = str(row.get("variable", "")).strip()
    model_path = Path(row.get("model_path", "")).as_posix()
    key = f"{mold}_{var}"

    try:
        model_file = BASE_DIR / Path(model_path)
        if not model_file.exists():
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_file}")

        with open(model_file, "rb") as f:
            model = joblib.load(f)

        arima_models[key] = {
            "model": model,
            "mold": mold,
            "variable": var,
            "p": int(row.get("p", 0)),
            "d": int(row.get("d", 0)),
            "q": int(row.get("q", 0)),
            "n_obs": int(row.get("n_obs_model", 0)),
            "aic": float(row.get("aic", 0)),
            "bic": float(row.get("bic", 0)),
            "cl": float(row.get("CL", 0)),
            "sigma": float(row.get("sigma", 0)),
            "ucl": float(row.get("UCL_3sigma", 0)),
            "lcl": float(row.get("LCL_3sigma", 0)),
        }

    except Exception as e:
        print(f"âš ï¸ [{key}] ë¡œë“œ ì‹¤íŒ¨ â†’ {e}")

print(f"âœ… ì´ {len(arima_models)}ê°œ ARIMA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")


# ====================================================
# 3ï¸âƒ£ ì ‘ê·¼ ìœ í‹¸ë¦¬í‹°
# ====================================================
def get_arima_model(mold_code: str, variable: str):
    """
    mold_codeì™€ variableë¡œ ARIMA ëª¨ë¸ ê²€ìƒ‰.
    ì˜ˆ:
        m = get_arima_model("8412", "Coolant_temperature")
        m["model"].forecast(steps=5)
    """
    key = f"{mold_code}_{variable}"
    return arima_models.get(key)


def list_arima_models():
    """í˜„ì¬ ë¡œë“œëœ ARIMA ëª¨ë¸ ì „ì²´ ëª©ë¡ ë°˜í™˜"""
    return pd.DataFrame([
        {
            "key": k,
            "mold": v["mold"],
            "variable": v["variable"],
            "p": v["p"],
            "d": v["d"],
            "q": v["q"]
        }
        for k, v in arima_models.items()
    ])
    
    
###  Xâ€“R ê´€ë¦¬ë„ ê¸°ì¤€ì„  ë¡œë“œ (ARIMA ì—†ëŠ” ë³€ìˆ˜ìš©)

def _load_xr_control_info(path: str = "./data/xr_control_info.csv") -> dict:
    """Xâ€“R ê´€ë¦¬ë„ìš© í†µê³„ì¹˜(XÌ„Ì„, RÌ„, UCL/LCL) ê³„ì‚°"""
    try:
        xr_df = pd.read_csv(path)
    except Exception as e:
        print(f"âš ï¸ Xâ€“R ê´€ë¦¬ë„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

    # Xâ€“R ê´€ë¦¬ë„ ìƒìˆ˜ í…Œì´ë¸” (ì„œë¸Œê·¸ë£¹ í¬ê¸° në³„)
    XR_CONSTANTS = {
        2: {"A2": 1.880, "D3": 0.000, "D4": 3.267},
        3: {"A2": 1.023, "D3": 0.000, "D4": 2.574},
        4: {"A2": 0.729, "D3": 0.000, "D4": 2.282},
        5: {"A2": 0.577, "D3": 0.000, "D4": 2.114},
        6: {"A2": 0.483, "D3": 0.000, "D4": 2.004},
        7: {"A2": 0.419, "D3": 0.076, "D4": 1.924},
        8: {"A2": 0.373, "D3": 0.136, "D4": 1.864},
        9: {"A2": 0.337, "D3": 0.184, "D4": 1.816},
        10: {"A2": 0.308, "D3": 0.223, "D4": 1.777},
    }

    xr_limits = {}

    for _, row in xr_df.iterrows():
        n = int(row["n"])
        const = XR_CONSTANTS.get(n, XR_CONSTANTS[5])  # ê¸°ë³¸ê°’ n=5
        A2, D3, D4 = const["A2"], const["D3"], const["D4"]

        cl_x = row["X_barbar"]
        cl_r = row["R_bar"]

        key = f"{row['mold_code']}_{row['variable']}"

        xr_limits[key] = {
            "mold": row["mold_code"],
            "variable": row["variable"],
            "n": n,
            "k_subgroups": row["k_subgroups"],
            "CL_X": cl_x,
            "UCL_X": cl_x + A2 * cl_r,
            "LCL_X": cl_x - A2 * cl_r,
            "CL_R": cl_r,
            "UCL_R": D4 * cl_r,
            "LCL_R": D3 * cl_r,
        }

    print(f"âœ… Xâ€“R ê´€ë¦¬ë„ ê¸°ì¤€ì„  ë¡œë“œ ì™„ë£Œ: {len(xr_limits)}ê°œ")
    return xr_limits


# ì „ì—­ ë³€ìˆ˜ë¡œ ë³´ê´€
xr_limits = _load_xr_control_info()